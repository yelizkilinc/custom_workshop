{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load CSVs (one-to-many) to Azure AI Search\n",
        "\n",
        "In this Jupyter Notebook, we create and run steps to index a CSV file in which each row is an individual and independent record/document. Each row then becomes searchable in Azure AI Search. \n",
        "The reference documentation can be found at [Indexing blobs and files to produce multiple search documents](https://learn.microsoft.com/en-us/azure/search/search-howto-index-one-to-many-blobs).\n",
        "\n",
        "By default, an indexer will treat the contents of a blob or file as a single search document. If you want a more granular representation in a search index, you can set `parsingMode` values to create multiple search documents from one blob or file.\n",
        "\n",
        "We are going to be using the same private Blob Storage account but a different container that has abstracts of 90k Medical publications about COVID-19 published in 2020-2022. This file is a subset of a much bigger dataset (770k articles) called CORD19 [HERE](https://github.com/allenai/cord19)"
      ],
      "metadata": {},
      "id": "4077b4ee-73ea-4155-afbd-1de66dd6b650"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import ManagedIdentityCredential\n",
        "\n",
        "load_dotenv(\"credentials.env\")\n",
        "\n",
        "# Name of the container in your Blob Storage Datasource ( in credentials.env)\n",
        "BLOB_CONTAINER_NAME = \"csv\""
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1727043599529
        }
      },
      "id": "c088c844-1e71-4279-a8fe-a77a007c15c4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the names for the data source, index and indexer\n",
        "datasource_name = \"srch-datasource-csv-yk\"\n",
        "skillset_name = \"srch-skillset-csv-yk\"\n",
        "index_name = \"srch-index-csv-yk\"\n",
        "indexer_name = \"srch-indexer-csv-yk\""
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1727043546361
        }
      },
      "id": "c4908539-1d17-46a3-b9e0-dcc46a210c4f"
    },
    {
      "cell_type": "code",
      "source": [
        "credential = ManagedIdentityCredential(client_id=\"d30cba06-04c1-4065-a91d-8b7ce3b07b78\")\n",
        "\n",
        "# Obtain an access token for the Azure Cognitive Search service\n",
        "token = credential.get_token(\"https://search.azure.com/.default\")\n",
        "\n",
        "print(f\"Access token: {token.token}\")\n",
        "\n",
        "# Construct the headers with the access token\n",
        "headers = {\n",
        "    'Content-Type': 'application/json',\n",
        "    'Authorization': f'Bearer {token.token}'\n",
        "    }\n",
        "\n",
        "params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Access token: eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6Ikg5bmo1QU9Tc3dNcGhnMVNGeDdqYVYtbEI5dyIsImtpZCI6Ikg5bmo1QU9Tc3dNcGhnMVNGeDdqYVYtbEI5dyJ9.eyJhdWQiOiJodHRwczovL3NlYXJjaC5henVyZS5jb20iLCJpc3MiOiJodHRwczovL3N0cy53aW5kb3dzLm5ldC8xNmIzYzAxMy1kMzAwLTQ2OGQtYWM2NC03ZWRhMDgyMGI2ZDMvIiwiaWF0IjoxNzI2OTg5MDc0LCJuYmYiOjE3MjY5ODkwNzQsImV4cCI6MTcyNzA3NTc3NCwiYWlvIjoiazJCZ1lOQTJqR3AwWDljNGYrV05VdUhsdVhkbEFRPT0iLCJhcHBpZCI6ImQzMGNiYTA2LTA0YzEtNDA2NS1hOTFkLThiN2NlM2IwN2I3OCIsImFwcGlkYWNyIjoiMiIsImlkcCI6Imh0dHBzOi8vc3RzLndpbmRvd3MubmV0LzE2YjNjMDEzLWQzMDAtNDY4ZC1hYzY0LTdlZGEwODIwYjZkMy8iLCJpZHR5cCI6ImFwcCIsIm9pZCI6ImNlN2VhNTVkLWFjZjktNDEwNy1hODkxLWNhMDRmNGQ0MTdiMyIsInJoIjoiMC5BVVlBRThDekZnRFRqVWFzWkg3YUNDQzIwNENqRFloZW1KaEJnYm5nV3h6Rk1WanhBQUEuIiwic3ViIjoiY2U3ZWE1NWQtYWNmOS00MTA3LWE4OTEtY2EwNGY0ZDQxN2IzIiwidGlkIjoiMTZiM2MwMTMtZDMwMC00NjhkLWFjNjQtN2VkYTA4MjBiNmQzIiwidXRpIjoiQkpOc0M3NU1DVVNfVVkyVG8tRVNBQSIsInZlciI6IjEuMCIsInhtc19pZHJlbCI6IjcgMTgiLCJ4bXNfbWlyaWQiOiIvc3Vic2NyaXB0aW9ucy9mZTM4YzM3Ni1iNDJhLTQ3NDEtOWU3Yy1mNWQ3YzMxZTU4NzMvcmVzb3VyY2Vncm91cHMveWVsaXpraWxpbmMtcmcvcHJvdmlkZXJzL01pY3Jvc29mdC5NYW5hZ2VkSWRlbnRpdHkvdXNlckFzc2lnbmVkSWRlbnRpdGllcy9NYW5hZ2VkSWRlbnRpdHlGb3JBTUwifQ.dQVkZLF9R0UzOyK1NbVehHVAKOgJD_SXR5JHYAqIQ1ac9-_43JjBSBIOfiUMJiXQacTfMWDYpmhTAtzX8qcT46aJBcDE8K1yFcbE0V6X66sflz4mlRkJjfTovsEP_AlAi4k_lGGIO5COiwlXciB_EqmeDj6ry-n7CIUo_clxt0Oz5DPvZmYDRnm2EvpW7saLqAOsCtsSiS0SYQ3R-LiWDRF8FVav2fI_E7FJ_pokSBvj7neUTmY62ZMVxR5dLbX_BAvEPV3iz86XIY3YJLi18BnsQciO45TYzAl_EzWSRVAc1JyzUmVo-Pa5dsdqcYtVmj7r2qvciXfjsu7nTnm2xw\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1727043603930
        }
      },
      "id": "d2e82369-5101-4b9d-b031-8975c69de16e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Data Source (Blob container with the CSV data file)"
      ],
      "metadata": {},
      "id": "ecad0e75-e3c8-4147-b8c6-b938435bc8f5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a data source\n",
        "\n",
        "datasource_payload = {\n",
        "    \"name\": datasource_name,\n",
        "    \"description\": \"Demo files to demonstrate cognitive search capabilities of one-to-many.\",\n",
        "    \"type\": \"azureblob\",\n",
        "    \"credentials\": {\n",
        "        \"connectionString\": os.environ['BLOB_CONNECTION_STRING']\n",
        "    },\n",
        "    \"container\": {\n",
        "        \"name\": BLOB_CONTAINER_NAME\n",
        "    }\n",
        "}\n",
        "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/datasources/\" + datasource_name,\n",
        "                 data=json.dumps(datasource_payload), headers=headers, params=params)\n",
        "print(r.status_code)\n",
        "print(r.ok)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "201\nTrue\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1727043691850
        }
      },
      "id": "a9fa6c09-a489-4b6d-8c93-5fc26bae63a0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect CSV file so we can understand the column types before creating the Index"
      ],
      "metadata": {},
      "id": "86b7ff86-19fc-48d3-88d1-b098e8d01302"
    },
    {
      "cell_type": "code",
      "source": [
        "#Read data from data store\n",
        "# Create data store and connect csv container \n",
        "#Follow the last step in the SET UP document\n",
        "import pandas as pd\n",
        "\n",
        "metadata = pd.read_csv(\"azureml://subscriptions/fe38c376-b42a-4741-9e7c-f5d7c31e5873/resourcegroups/yelizkilinc-rg/workspaces/aml-prod/datastores/aibootcamp/paths/all_sources_metadata.csv\")\n",
        "print(\"No. of lines:\",metadata.shape[0])\n",
        "metadata.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "4d5bebe3-87fb-446c-89fa-0030c43025a7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our private dataset we have place a smaller version of the original the metadata.csv file in the cord19 dataset. \n",
        "Let's see what the file looks like:"
      ],
      "metadata": {},
      "id": "6cf6879a-a3da-4e54-97ed-f4122325abb1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Index\n",
        "In Azure AI Search, both blob indexers and file indexers support a delimitedText parsing mode for CSV files that treats each line in the CSV as a separate search document.\n",
        "\n",
        "### **Important**:\n",
        "As you can see below and from the prior Notebook, there are 6 mandatory fields in the schema: `id, title, name, location, chunk, chunkVector`. These fields must exist in any index that you create regardles of the datasource. Any additional fields are good to add so the engine can search relevant documents, however the mandatory fields must exist for all the code downstream work with no issues."
      ],
      "metadata": {
        "tags": []
      },
      "id": "0a321916-cd14-4d34-837d-1d153edb1221"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an index\n",
        "# Queries operate over the searchable fields and filterable fields in the index\n",
        "index_payload = {\n",
        "    \"name\": index_name,\n",
        "    \"vectorSearch\": {\n",
        "        \"algorithms\": [\n",
        "            {\n",
        "                \"name\": \"myalgo\",\n",
        "                \"kind\": \"hnsw\"\n",
        "            }\n",
        "        ],\n",
        "        \"vectorizers\": [\n",
        "            {\n",
        "                \"name\": \"openai\",\n",
        "                \"kind\": \"azureOpenAI\",\n",
        "                \"azureOpenAIParameters\":\n",
        "                {\n",
        "                    \"resourceUri\" : os.environ['AZURE_OPENAI_ENDPOINT'],\n",
        "                    \"apiKey\" : os.environ['AZURE_OPENAI_API_KEY'],\n",
        "                    \"deploymentId\" : os.environ['EMBEDDING_DEPLOYMENT_NAME'],\n",
        "                    \"modelName\" : os.environ['EMBEDDING_DEPLOYMENT_NAME'],\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        \"profiles\": [\n",
        "            {\n",
        "                \"name\": \"myprofile\",\n",
        "                \"algorithm\": \"myalgo\",\n",
        "                \"vectorizer\":\"openai\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"semantic\": {\n",
        "        \"configurations\": [\n",
        "            {\n",
        "                \"name\": \"my-semantic-config\",\n",
        "                \"prioritizedFields\": {\n",
        "                    \"titleField\": {\n",
        "                        \"fieldName\": \"title\"\n",
        "                    },\n",
        "                    \"prioritizedContentFields\": [\n",
        "                        {\n",
        "                            \"fieldName\": \"chunk\"\n",
        "                        }\n",
        "                    ],\n",
        "                    \"prioritizedKeywordsFields\": []\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"fields\": [\n",
        "        {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": \"true\", \"analyzer\": \"keyword\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\",\"facetable\": \"false\"},\n",
        "        {\"name\": \"ParentKey\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"facetable\": \"false\", \"filterable\": \"true\", \"sortable\": \"false\"},\n",
        "        {\"name\": \"title\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"facetable\": \"false\", \"filterable\": \"true\", \"sortable\": \"false\"},\n",
        "        {\"name\": \"name\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
        "        {\"name\": \"location\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},   \n",
        "        {\"name\": \"chunk\",\"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
        "        {\n",
        "            \"name\": \"chunkVector\",\n",
        "            \"type\": \"Collection(Edm.Single)\",\n",
        "            \"dimensions\": 1536,   # IMPORTANT: Make sure these dimmensions match your embedding model name\n",
        "            \"vectorSearchProfile\": \"myprofile\",\n",
        "            \"searchable\": \"true\",\n",
        "            \"retrievable\": \"true\",\n",
        "            \"filterable\": \"false\",\n",
        "            \"sortable\": \"false\",\n",
        "            \"facetable\": \"false\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name,\n",
        "                 data=json.dumps(index_payload), headers=headers, params=params)\n",
        "print(r.status_code)\n",
        "print(r.ok)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "201\nTrue\n"
        }
      ],
      "execution_count": 7,
      "metadata": {},
      "id": "74913764-9dfb-4646-aac8-d389cd4533e6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Skillset - Text Splitter, Language Detection\n",
        "We don't need to use the OCR and Merge skill, since we know for sure that this is only text parsing. We will only use the SplitSkill and the AzureOpenAIEmbeddingSkill."
      ],
      "metadata": {},
      "id": "c0b3935d-8546-4756-95cd-7f4fcecb9836"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a skillset\n",
        "skillset_payload = {\n",
        "    \"name\": skillset_name,\n",
        "    \"description\": \"e2e Skillset for RAG - One to Many\",\n",
        "    \"skills\":\n",
        "    [\n",
        "        {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Text.SplitSkill\",\n",
        "            \"context\": \"/document\",\n",
        "            \"textSplitMode\": \"pages\",\n",
        "            \"maximumPageLength\": 5000, # 5000 is default\n",
        "            \"defaultLanguageCode\": \"en\",\n",
        "            \"inputs\": [\n",
        "                {\n",
        "                    \"name\": \"text\",\n",
        "                    \"source\": \"/document/abstract\"\n",
        "                }\n",
        "            ],\n",
        "            \"outputs\": [\n",
        "                {\n",
        "                    \"name\": \"textItems\",\n",
        "                    \"targetName\": \"chunks\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Text.AzureOpenAIEmbeddingSkill\",\n",
        "            \"description\": \"Azure OpenAI Embedding Skill\",\n",
        "            \"context\": \"/document/chunks/*\",\n",
        "            \"resourceUri\": os.environ['AZURE_OPENAI_ENDPOINT'],\n",
        "            \"apiKey\": os.environ['AZURE_OPENAI_API_KEY'],\n",
        "            \"deploymentId\": os.environ['EMBEDDING_DEPLOYMENT_NAME'],\n",
        "            \"modelName\" : os.environ['EMBEDDING_DEPLOYMENT_NAME'],\n",
        "            \"inputs\": [\n",
        "                {\n",
        "                    \"name\": \"text\",\n",
        "                    \"source\": \"/document/chunks/*\"\n",
        "                }\n",
        "            ],\n",
        "            \"outputs\": [\n",
        "                {\n",
        "                    \"name\": \"embedding\",\n",
        "                    \"targetName\": \"vector\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    \"indexProjections\": {\n",
        "        \"selectors\": [\n",
        "            {\n",
        "                \"targetIndexName\": index_name,\n",
        "                \"parentKeyFieldName\": \"ParentKey\",\n",
        "                \"sourceContext\": \"/document/chunks/*\",\n",
        "                \"mappings\": [\n",
        "                    {\n",
        "                        \"name\": \"title\",\n",
        "                        \"source\": \"/document/title\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"name\": \"name\",\n",
        "                        \"source\": \"/document/name\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"name\": \"location\",\n",
        "                        \"source\": \"/document/location\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"name\": \"chunk\",\n",
        "                        \"source\": \"/document/chunks/*\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"name\": \"chunkVector\",\n",
        "                        \"source\": \"/document/chunks/*/vector\"\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"parameters\": {\n",
        "            \"projectionMode\": \"skipIndexingParentDocuments\"\n",
        "        }\n",
        "    },\n",
        "    \"cognitiveServices\": {\n",
        "        \"@odata.type\": \"#Microsoft.Azure.Search.CognitiveServicesByKey\",\n",
        "        \"description\": os.environ['COG_SERVICES_NAME'],\n",
        "        \"key\": os.environ['COG_SERVICES_KEY']\n",
        "    }\n",
        "}\n",
        "\n",
        "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/skillsets/\" + skillset_name,\n",
        "                 data=json.dumps(skillset_payload), headers=headers, params=params)\n",
        "print(r.status_code)\n",
        "print(r.ok)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "201\nTrue\n"
        }
      ],
      "execution_count": 8,
      "metadata": {},
      "id": "b46cfa90-28b4-4602-b6ff-743a3407fd72"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and Run the Indexer - (runs the pipeline)\n",
        "To create one-to-many indexers with CSV blobs, create or update an indexer definition with the delimitedText parsing mode"
      ],
      "metadata": {},
      "id": "51849738-6f66-452a-b7df-d34afd11f943"
    },
    {
      "cell_type": "code",
      "source": [
        "indexer_payload = {\n",
        "    \"name\": indexer_name,\n",
        "    \"dataSourceName\": datasource_name,\n",
        "    \"targetIndexName\": index_name,\n",
        "    \"skillsetName\": skillset_name,\n",
        "    \"schedule\" : { \"interval\" : \"PT30M\"}, # How often do you want to check for new content in the data source\n",
        "    \"fieldMappings\": [\n",
        "        {\n",
        "          \"sourceFieldName\" : \"metadata_storage_name\",\n",
        "          \"targetFieldName\" : \"name\"\n",
        "        },\n",
        "        {\n",
        "          \"sourceFieldName\" : \"url\",\n",
        "          \"targetFieldName\" : \"location\"\n",
        "        }\n",
        "    ],\n",
        "    \"outputFieldMappings\":\n",
        "    [],\n",
        "    \"parameters\" : { \n",
        "        \"configuration\" : { \n",
        "            \"dataToExtract\": \"contentAndMetadata\",\n",
        "            \"parsingMode\" : \"delimitedText\", \n",
        "            \"firstLineContainsHeaders\" : True,\n",
        "            \"delimitedTextDelimiter\": \",\"\n",
        "        } \n",
        "    }\n",
        "}\n",
        "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexers/\" + indexer_name,\n",
        "                 data=json.dumps(indexer_payload), headers=headers, params=params)\n",
        "print(r.status_code)\n",
        "print(r.ok)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "201\nTrue\n"
        }
      ],
      "execution_count": 9,
      "metadata": {},
      "id": "b87b8ebd-8091-43b6-9124-cc17021cfb78"
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally, get indexer status to confirm that it's running\n",
        "try:\n",
        "    r = requests.get(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexers/\" + indexer_name +\n",
        "                     \"/status\", headers=headers, params=params)\n",
        "    # pprint(json.dumps(r.json(), indent=1))\n",
        "    print(r.status_code)\n",
        "    print(\"Status:\",r.json().get('lastResult').get('status'))\n",
        "    print(\"Items Processed:\",r.json().get('lastResult').get('itemsProcessed'))\n",
        "    print(r.ok)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(\"Wait a few seconds until the process starts and run this cell again.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "200\nStatus: inProgress\nItems Processed: 4268\nTrue\n"
        }
      ],
      "execution_count": 15,
      "metadata": {},
      "id": "6132c041-7213-410e-a206-1a8c7385128e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When the indexer finishes running we will have all 90,000 rows indexed properly as separate documents in our Search Engine!.**"
      ],
      "metadata": {},
      "id": "2152806f-245c-45db-93c6-c19c0569d73a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n",
        "- https://learn.microsoft.com/en-us/azure/search/search-howto-index-csv-blobs\n",
        "\n"
      ],
      "metadata": {},
      "id": "0eed6f22-437f-4a49-9b67-5fa2e7d066bf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEXT\n",
        "Now that we have two separate indexes loaded with two different types of information, In the next notebook 3, we will do a Multi-Index query, sort the results based on the reranker semantic score of Azure AI Search, and then use OpenAI to understand the results and give the best answer possible"
      ],
      "metadata": {},
      "id": "4d9f82a9-cb4c-44b9-b125-bc124ea23aa8"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "7505d8f9-39c7-4b87-a85f-283b6fea3de0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}