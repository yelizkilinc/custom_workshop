{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Welcome to this repository. We will be walking you to a series of notebooks in which you will understand how RAG works (Retrieval Augmented Generation, a technique that combines the power of search and generation of AI to answer user queries). We will work with different sources (Azure AI Search, Files, SQL Server, Websites, APIs, etc) and at the end of the notebooks you will understand why the magic happens with the combination of:\n",
        "\n",
        "1) Multi-Agents: Agents talking to each other\n",
        "2) Azure OpenAI models\n",
        "3) Very detailed prompts\n",
        "\n",
        "But we need to start from the basics, so let's begin with Azure AI Search and how it works:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Enrich multiple file types Azure AI Search\n",
        "\n",
        "In this Jupyter Notebook, we create and run enrichment steps to unlock searchable content in the specified Azure blob. It performs operations over mixed content in Azure Storage, such as images and application files, using a skillset that analyzes and extracts text information that becomes searchable in Azure Cognitive Search. \n",
        "The reference sample can be found at [Tutorial: Use Python and AI to generate searchable content from Azure blobs](https://docs.microsoft.com/azure/search/cognitive-search-tutorial-blob-python).\n",
        "\n",
        "In this demo we are going to be using a private (so we can mimic a private data lake scenario) Blob Storage container that has ~9.8k Computer Science publication PDFs from the Arxiv dataset.\n",
        "https://www.kaggle.com/datasets/Cornell-University/arxiv\n",
        "\n",
        "If you want to explore the dataset, go [HERE](https://console.cloud.google.com/storage/browser/arxiv-dataset/arxiv/cs/pdf?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false)<br>\n",
        "Note: This dataset has been copy to a public azure blob container for this demo\n",
        "\n",
        "Although only  PDF files are used here, this can be done at a much larger scale and Azure Cognitive Search supports a range of other file formats including: Microsoft Office (DOCX/DOC, XSLX/XLS, PPTX/PPT, MSG), HTML, XML, ZIP, and plain text files (including JSON).\n",
        "Azure Search support the following sources: [Data Sources Gallery](https://learn.microsoft.com/EN-US/AZURE/search/search-data-sources-gallery)\n",
        "\n",
        "This notebook creates the following objects on your search service:\n",
        "\n",
        "+ data source\n",
        "+ search index\n",
        "+ skillset\n",
        "+ indexer\n",
        "\n",
        "This notebook calls the [Search REST APIs](https://docs.microsoft.com/rest/api/searchservice/), but you can also use the Azure.Search.Documents client library in the Azure SDK for Python to perform the same steps. See this [Python quickstart](https://docs.microsoft.com/azure/search/search-get-started-python) for details.\n",
        "\n",
        "To run this notebook, you should have already created the Azure services on README. Once you've done this, you can run all cells, but the query won't return results until the indexer is finished and the search index is loaded. \n",
        "\n",
        "We recommend running each step and making sure it completes before moving on."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cog-search](./images/Cog-Search-Enrich.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import ManagedIdentityCredential\n",
        "\n",
        "load_dotenv(\"credentials.env\")\n",
        "\n",
        "# Name of the container in your Blob Storage Datasource ( in credentials.env)\n",
        "BLOB_CONTAINER_NAME = \"cord19\""
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1726989371409
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the names for the data source, skillset, index and indexer\n",
        "datasource_name = \"srch-datasource-files-yk\"\n",
        "index_name = \"srch-index-files-yk\"\n",
        "skillset_name = \"srch-skillset-files-yk\"\n",
        "indexer_name = \"srch-indexer-files-yk\""
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1726989372068
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "credential = ManagedIdentityCredential(client_id=\"d30cba06-04c1-4065-a91d-8b7ce3b07b78\")\n",
        "\n",
        "# Obtain an access token for the Azure Cognitive Search service\n",
        "token = credential.get_token(\"https://search.azure.com/.default\")\n",
        "\n",
        "print(f\"Access token: {token.token}\")\n",
        "\n",
        "# Construct the headers with the access token\n",
        "headers = {\n",
        "    'Content-Type': 'application/json',\n",
        "    'Authorization': f'Bearer {token.token}'\n",
        "    }\n",
        "\n",
        "params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Access token: eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6Ikg5bmo1QU9Tc3dNcGhnMVNGeDdqYVYtbEI5dyIsImtpZCI6Ikg5bmo1QU9Tc3dNcGhnMVNGeDdqYVYtbEI5dyJ9.eyJhdWQiOiJodHRwczovL3NlYXJjaC5henVyZS5jb20iLCJpc3MiOiJodHRwczovL3N0cy53aW5kb3dzLm5ldC8xNmIzYzAxMy1kMzAwLTQ2OGQtYWM2NC03ZWRhMDgyMGI2ZDMvIiwiaWF0IjoxNzI2OTg5MDc0LCJuYmYiOjE3MjY5ODkwNzQsImV4cCI6MTcyNzA3NTc3NCwiYWlvIjoiazJCZ1lOQTJqR3AwWDljNGYrV05VdUhsdVhkbEFRPT0iLCJhcHBpZCI6ImQzMGNiYTA2LTA0YzEtNDA2NS1hOTFkLThiN2NlM2IwN2I3OCIsImFwcGlkYWNyIjoiMiIsImlkcCI6Imh0dHBzOi8vc3RzLndpbmRvd3MubmV0LzE2YjNjMDEzLWQzMDAtNDY4ZC1hYzY0LTdlZGEwODIwYjZkMy8iLCJpZHR5cCI6ImFwcCIsIm9pZCI6ImNlN2VhNTVkLWFjZjktNDEwNy1hODkxLWNhMDRmNGQ0MTdiMyIsInJoIjoiMC5BVVlBRThDekZnRFRqVWFzWkg3YUNDQzIwNENqRFloZW1KaEJnYm5nV3h6Rk1WanhBQUEuIiwic3ViIjoiY2U3ZWE1NWQtYWNmOS00MTA3LWE4OTEtY2EwNGY0ZDQxN2IzIiwidGlkIjoiMTZiM2MwMTMtZDMwMC00NjhkLWFjNjQtN2VkYTA4MjBiNmQzIiwidXRpIjoiQkpOc0M3NU1DVVNfVVkyVG8tRVNBQSIsInZlciI6IjEuMCIsInhtc19pZHJlbCI6IjcgMTgiLCJ4bXNfbWlyaWQiOiIvc3Vic2NyaXB0aW9ucy9mZTM4YzM3Ni1iNDJhLTQ3NDEtOWU3Yy1mNWQ3YzMxZTU4NzMvcmVzb3VyY2Vncm91cHMveWVsaXpraWxpbmMtcmcvcHJvdmlkZXJzL01pY3Jvc29mdC5NYW5hZ2VkSWRlbnRpdHkvdXNlckFzc2lnbmVkSWRlbnRpdGllcy9NYW5hZ2VkSWRlbnRpdHlGb3JBTUwifQ.dQVkZLF9R0UzOyK1NbVehHVAKOgJD_SXR5JHYAqIQ1ac9-_43JjBSBIOfiUMJiXQacTfMWDYpmhTAtzX8qcT46aJBcDE8K1yFcbE0V6X66sflz4mlRkJjfTovsEP_AlAi4k_lGGIO5COiwlXciB_EqmeDj6ry-n7CIUo_clxt0Oz5DPvZmYDRnm2EvpW7saLqAOsCtsSiS0SYQ3R-LiWDRF8FVav2fI_E7FJ_pokSBvj7neUTmY62ZMVxR5dLbX_BAvEPV3iz86XIY3YJLi18BnsQciO45TYzAl_EzWSRVAc1JyzUmVo-Pa5dsdqcYtVmj7r2qvciXfjsu7nTnm2xw\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726989373456
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Data Source (Blob container with the Arxiv CS pdfs)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code sends the json paylod to Azure Search engine to create the Datasource\n",
        "\n",
        "datasource_payload = {\n",
        "    \"name\": datasource_name,\n",
        "    \"description\": \"Demo files to demonstrate cognitive search capabilities.\",\n",
        "    \"type\": \"azureblob\",\n",
        "    \"credentials\": {\n",
        "        \"connectionString\": os.environ['BLOB_CONNECTION_STRING']\n",
        "    },\n",
        "    \"dataDeletionDetectionPolicy\" : {\n",
        "        \"@odata.type\" :\"#Microsoft.Azure.Search.NativeBlobSoftDeleteDeletionDetectionPolicy\" # this makes sure that if the item is deleted from the source, it will be deleted from the index\n",
        "    },\n",
        "    \"container\": {\n",
        "        \"name\": BLOB_CONTAINER_NAME\n",
        "    }\n",
        "}\n",
        "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/datasources/\" + datasource_name,\n",
        "                 data=json.dumps(datasource_payload), headers=headers, params=params)\n",
        "print(r.status_code)\n",
        "print(r.ok)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "204\nTrue\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1726989378706
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 201 - Successfully created\n",
        "- 204 - Succesfully overwritten\n",
        "- 40X - Authentication Error\n",
        "\n",
        "For information on Change and Delete file detection please see [HERE](https://learn.microsoft.com/en-us/azure/search/search-howto-index-changed-deleted-blobs?tabs=rest-api)\n",
        "\n",
        "Also, if your data is one AWS or GCP, and do not want to move it to Azure, you can create a Azure Fabric shortcut in OneLake, and use Fabric as a datasource here. From the documentation [HERE](https://learn.microsoft.com/en-us/azure/search/search-how-to-index-onelake-files):\n",
        "> If you use Microsoft Fabric and OneLake for data access to Amazon Web Services (AWS) and Google data sources, use this indexer to import external data into a search index. This indexer is available through the Azure portal, the 2024-05-01-preview REST API, and Azure SDK beta packages."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# If you have a 403 code, probably you have a wrong endpoint or key, you can debug by uncomment this\n",
        "r.text"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "''"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1726750424293
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Index"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Azure AI Search, a search index is your searchable content, available to the search engine for indexing, full text search, vector search, hybrid search, and filtered queries. An index is defined by a schema and saved to the search service, with data import following as a second step. This content exists within your search service, apart from your primary data stores, which is necessary for the millisecond response times expected in modern search applications. Except for indexer-driven indexing scenarios, the search service never connects to or queries your source data.\n",
        "\n",
        "Reference:\n",
        "\n",
        "https://learn.microsoft.com/en-us/azure/search/search-what-is-an-index"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice below how we are creating a vector store. In Azure AI Search, a vector store has an index schema that defines vector and nonvector fields, a vector configuration for algorithms that create the embedding space, and settings on vector field definitions that are used in query requests. \n",
        "\n",
        "We are also setting a semantic ranking over a result set, promoting the most semantically relevant results to the top of the stack. You can also get semantic captions, with highlights over the most relevant terms and phrases, and semantic answers."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import ManagedIdentityCredential\n",
        "\n",
        "credential = ManagedIdentityCredential(client_id=\"f4980c43-9766-48d7-a925-c377a74605bb\")\n",
        "token=credential.get_token(\"https://cognitiveservices.azure.com/\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "AzureMLCredential.get_token failed: Unexpected content type \"text/plain; charset=utf-8\"\nContent: InternalError - :{\nInfo: Request failure status code: 404\n\n}\nManagedIdentityCredential.get_token failed: Unexpected content type \"text/plain; charset=utf-8\"\nContent: InternalError - :{\nInfo: Request failure status code: 404\n\n}\n"
        },
        {
          "output_type": "error",
          "ename": "ClientAuthenticationError",
          "evalue": "Unexpected content type \"text/plain; charset=utf-8\"\nContent: InternalError - :{\nInfo: Request failure status code: 404\n\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/pipeline/policies/_universal.py:607\u001b[0m, in \u001b[0;36mContentDecodePolicy.deserialize_from_text\u001b[0;34m(cls, data, mime_type, response)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_as_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDecodeError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/identity/_internal/managed_identity_client.py:42\u001b[0m, in \u001b[0;36mManagedIdentityClientBase._process_response\u001b[0;34m(self, response, request_time)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[43mContentDecodePolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_from_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/pipeline/policies/_universal.py:609\u001b[0m, in \u001b[0;36mContentDecodePolicy.deserialize_from_text\u001b[0;34m(cls, data, mime_type, response)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 609\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m DecodeError(\n\u001b[1;32m    610\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJSON is invalid: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(err),\n\u001b[1;32m    611\u001b[0m             response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m    612\u001b[0m             error\u001b[38;5;241m=\u001b[39merr,\n\u001b[1;32m    613\u001b[0m         )\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxml\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m (mime_type \u001b[38;5;129;01mor\u001b[39;00m []):\n",
            "\u001b[0;31mDecodeError\u001b[0m: JSON is invalid: Expecting value: line 1 column 1 (char 0)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mClientAuthenticationError\u001b[0m                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ManagedIdentityCredential\n\u001b[1;32m      3\u001b[0m credential \u001b[38;5;241m=\u001b[39m ManagedIdentityCredential(client_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf4980c43-9766-48d7-a925-c377a74605bb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m token\u001b[38;5;241m=\u001b[39m\u001b[43mcredential\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://cognitiveservices.azure.com/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/identity/_internal/decorators.py:32\u001b[0m, in \u001b[0;36mlog_get_token.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m         _LOGGER\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m     34\u001b[0m             logging\u001b[38;5;241m.\u001b[39mDEBUG \u001b[38;5;28;01mif\u001b[39;00m within_credential_chain\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01melse\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m succeeded\u001b[39m\u001b[38;5;124m\"\u001b[39m, qualified_name\n\u001b[1;32m     35\u001b[0m         )\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _LOGGER\u001b[38;5;241m.\u001b[39misEnabledFor(logging\u001b[38;5;241m.\u001b[39mDEBUG):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/identity/_credentials/managed_identity.py:128\u001b[0m, in \u001b[0;36mManagedIdentityCredential.get_token\u001b[0;34m(self, *scopes, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credential:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CredentialUnavailableError(\n\u001b[1;32m    123\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo managed identity endpoint found. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Target Azure platform could not be determined from environment variables. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisit https://aka.ms/azsdk/python/identity/managedidentitycredential/troubleshoot to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtroubleshoot this issue.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m     )\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_credential\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/identity/_internal/managed_identity_base.py:45\u001b[0m, in \u001b[0;36mManagedIdentityBase.get_token\u001b[0;34m(self, *scopes, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CredentialUnavailableError(message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_unavailable_message())\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mManagedIdentityBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/identity/_internal/get_token_mixin.py:64\u001b[0m, in \u001b[0;36mGetTokenMixin.get_token\u001b[0;34m(self, *scopes, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_request_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m---> 64\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_refresh(token):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/identity/_internal/managed_identity_base.py:55\u001b[0m, in \u001b[0;36mManagedIdentityBase._request_token\u001b[0;34m(self, *scopes, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mscopes: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AccessToken:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mManagedIdentityClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/identity/_internal/managed_identity_client.py:118\u001b[0m, in \u001b[0;36mManagedIdentityClient.request_token\u001b[0;34m(self, *scopes, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m request_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m    117\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline\u001b[38;5;241m.\u001b[39mrun(request, retry_on_methods\u001b[38;5;241m=\u001b[39m[request\u001b[38;5;241m.\u001b[39mmethod], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/identity/_internal/managed_identity_client.py:50\u001b[0m, in \u001b[0;36mManagedIdentityClientBase._process_response\u001b[0;34m(self, response, request_time)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m             message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected content type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(response\u001b[38;5;241m.\u001b[39mhttp_response\u001b[38;5;241m.\u001b[39mcontent_type)\n\u001b[0;32m---> 50\u001b[0m         \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mClientAuthenticationError\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_response\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m content:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientAuthenticationError(message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo token received.\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mhttp_response)\n",
            "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mClientAuthenticationError\u001b[0m: Unexpected content type \"text/plain; charset=utf-8\"\nContent: InternalError - :{\nInfo: Request failure status code: 404\n\n}"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1726990062051
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an index\n",
        "# Queries operate over the searchable fields and filterable fields in the index\n",
        "index_payload = {\n",
        "    \"name\": index_name,\n",
        "    \"vectorSearch\": {\n",
        "        \"algorithms\": [\n",
        "            {\n",
        "                \"name\": \"myalgo\",\n",
        "                \"kind\": \"hnsw\"\n",
        "            }\n",
        "        ],\n",
        "        \"vectorizers\": [\n",
        "            {\n",
        "                \"name\": \"openai\",\n",
        "                \"kind\": \"azureOpenAI\",\n",
        "                \"azureOpenAIParameters\":\n",
        "                {\n",
        "                    \"resourceUri\" : os.environ['AZURE_OPENAI_ENDPOINT'],\n",
        "                    #\"apiKey\" :\"\",\n",
        "                    \"deploymentId\" : os.environ['EMBEDDING_DEPLOYMENT_NAME'],\n",
        "                    \"modelName\" : os.environ['EMBEDDING_DEPLOYMENT_NAME'],\n",
        "                    \"authIdentity\": None\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        \"profiles\": [\n",
        "            {\n",
        "                \"name\": \"myprofile\",\n",
        "                \"algorithm\": \"myalgo\",\n",
        "                \"vectorizer\":\"openai\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"semantic\": {\n",
        "        \"configurations\": [\n",
        "            {\n",
        "                \"name\": \"my-semantic-config\",\n",
        "                \"prioritizedFields\": {\n",
        "                    \"titleField\": {\n",
        "                        \"fieldName\": \"title\"\n",
        "                    },\n",
        "                    \"prioritizedContentFields\": [\n",
        "                        {\n",
        "                            \"fieldName\": \"chunk\"\n",
        "                        }\n",
        "                    ],\n",
        "                    \"prioritizedKeywordsFields\": []\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"fields\": [\n",
        "        {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": \"true\", \"analyzer\": \"keyword\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\",\"facetable\": \"false\"},\n",
        "        {\"name\": \"ParentKey\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"facetable\": \"false\", \"filterable\": \"true\", \"sortable\": \"false\"},\n",
        "        {\"name\": \"title\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"facetable\": \"false\", \"filterable\": \"true\", \"sortable\": \"false\"},\n",
        "        {\"name\": \"name\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
        "        {\"name\": \"location\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},   \n",
        "        {\"name\": \"chunk\",\"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
        "        {\n",
        "            \"name\": \"chunkVector\",\n",
        "            \"type\": \"Collection(Edm.Single)\",\n",
        "            \"dimensions\": 1536, # IMPORTANT: Make sure these dimmensions match your embedding model name\n",
        "            \"vectorSearchProfile\": \"myprofile\",\n",
        "            \"searchable\": \"true\",\n",
        "            \"retrievable\": \"true\",\n",
        "            \"filterable\": \"false\",\n",
        "            \"sortable\": \"false\",\n",
        "            \"facetable\": \"false\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name,\n",
        "                 data=json.dumps(index_payload), headers=headers, params=params)\n",
        "print(r.status_code)\n",
        "print(r.ok)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "201\nTrue\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1726992925436
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r.text"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "'{\"error\":{\"code\":\"\",\"message\":\"The request is invalid. Details: Cannot dynamically create an instance of type \\'Microsoft.Azure.Search.DataIdentity\\'. Reason: Cannot create an abstract class.\"}}'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1726992795736
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Semantic Search capabilities\n",
        "As you can see above in the index payload, there is a `semantic configuration`. What is that?\n",
        "\n",
        "Semantic ranker is a collection of query-related capabilities that improve the quality of an initial BM25-ranked or RRF-ranked search result for text-based queries. When you enable it on your search service, semantic ranking extends the query execution pipeline in two ways:\n",
        "\n",
        "    First, it adds secondary ranking over an initial result set that was scored using BM25 or RRF. This secondary ranking uses multi-lingual, deep learning models adapted from Microsoft Bing to promote the most semantically relevant results.\n",
        "\n",
        "    Second, it extracts and returns captions and answers in the response, which you can render on a search page to improve the user's search experience.\n",
        "\n",
        "\n",
        "For deeper explanation and limitations see [HERE](https://learn.microsoft.com/en-us/azure/search/semantic-ranking)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Skillset - OCR, Text Splitter, AzureOpenAIEmbeddingSkill\n",
        "\n",
        "We need to create now the skillset. This is a set of steps in which we use AI Services to enrich the documents by extracting information, applying OCR, splitting, and embedding chunks, among other skills.\n",
        "\n",
        "https://learn.microsoft.com/en-us/azure/search/cognitive-search-working-with-skillsets\n",
        "\n",
        "https://learn.microsoft.com/en-us/azure/search/cognitive-search-predefined-skills\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice below that we are using IndexProjections. By default, one document processed within a skillset maps to a single document in the search index. This means that if you perform chunking of an input text and then perform enrichments on each chunk, the result in the index when mapped via outputFieldMappings is an array of the generated enrichments. **With index projections, you define a context at which to map each chunk of enriched data to its own search document**. This allows you to apply a one-to-many mapping of a document's enriched data to the search index.\n",
        "    \n",
        "The parameter: `\"projectionMode\": \"skipIndexingParentDocuments\"` allows us to skip the indexing of the parent documents, and keep only the index with the chunks and its vectors.\n",
        "\n",
        "### Content Lifecycle\n",
        "If the indexer data source supports change tracking and deletion detection, the indexing process can synchronize the primary (parend documents) and secondary indexes (chunks) to pick up those changes.\n",
        "\n",
        "Each time you run the indexer and skillset, the index projections are updated if the skillset or underlying source data has changed. Any changes picked up by the indexer are propagated through the enrichment process to the projections in the index, ensuring that your projected data is a current representation of content in the originating data source. This will save you weeks of programming and a lot of headaches trying to keep the content in sync."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a skillset\n",
        "skillset_payload = {\n",
        "    \"name\": skillset_name,\n",
        "    \"description\": \"e2e Skillset for RAG - Files\",\n",
        "    \"skills\":\n",
        "    [\n",
        "        {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Vision.OcrSkill\",\n",
        "            \"description\": \"Extract text (plain and structured) from image.\",\n",
        "            \"context\": \"/document/normalized_images/*\",\n",
        "            \"defaultLanguageCode\": \"en\",\n",
        "            \"detectOrientation\": True,\n",
        "            \"inputs\": [\n",
        "                {\n",
        "                  \"name\": \"image\",\n",
        "                  \"source\": \"/document/normalized_images/*\"\n",
        "                }\n",
        "            ],\n",
        "                \"outputs\": [\n",
        "                {\n",
        "                  \"name\": \"text\",\n",
        "                  \"targetName\" : \"images_text\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Text.MergeSkill\",\n",
        "            \"description\": \"Create merged_text, which includes all the textual representation of each image inserted at the right location in the content field. This is useful for PDF and other file formats that supported embedded images.\",\n",
        "            \"context\": \"/document\",\n",
        "            \"insertPreTag\": \" \",\n",
        "            \"insertPostTag\": \" \",\n",
        "            \"inputs\": [\n",
        "                {\n",
        "                  \"name\":\"text\", \"source\": \"/document/content\"\n",
        "                },\n",
        "                {\n",
        "                  \"name\": \"itemsToInsert\", \"source\": \"/document/normalized_images/*/images_text\"\n",
        "                },\n",
        "                {\n",
        "                  \"name\":\"offsets\", \"source\": \"/document/normalized_images/*/contentOffset\"\n",
        "                }\n",
        "            ],\n",
        "            \"outputs\": [\n",
        "                {\n",
        "                  \"name\": \"mergedText\", \n",
        "                  \"targetName\" : \"merged_text\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Text.SplitSkill\",\n",
        "            \"context\": \"/document\",\n",
        "            \"textSplitMode\": \"pages\",  # although it says \"pages\" it actally means chunks, not actual pages\n",
        "            \"maximumPageLength\": 5000, # 5000 characters is default and a good choice\n",
        "            \"pageOverlapLength\": 750,  # 15% overlap among chunks\n",
        "            \"defaultLanguageCode\": \"en\",\n",
        "            \"inputs\": [\n",
        "                {\n",
        "                    \"name\": \"text\",\n",
        "                    \"source\": \"/document/merged_text\"\n",
        "                }\n",
        "            ],\n",
        "            \"outputs\": [\n",
        "                {\n",
        "                    \"name\": \"textItems\",\n",
        "                    \"targetName\": \"chunks\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Text.AzureOpenAIEmbeddingSkill\",\n",
        "            \"description\": \"Azure OpenAI Embedding Skill\",\n",
        "            \"context\": \"/document/chunks/*\",\n",
        "            \"resourceUri\": os.environ['AZURE_OPENAI_ENDPOINT'],\n",
        "            \"apiKey\": os.environ['AZURE_OPENAI_API_KEY'],\n",
        "            \"deploymentId\": os.environ['EMBEDDING_DEPLOYMENT_NAME'],\n",
        "            \"modelName\": os.environ['EMBEDDING_DEPLOYMENT_NAME'],\n",
        "            \"inputs\": [\n",
        "                {\n",
        "                    \"name\": \"text\",\n",
        "                    \"source\": \"/document/chunks/*\"\n",
        "                }\n",
        "            ],\n",
        "            \"outputs\": [\n",
        "                {\n",
        "                    \"name\": \"embedding\",\n",
        "                    \"targetName\": \"vector\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    \"indexProjections\": {\n",
        "        \"selectors\": [\n",
        "            {\n",
        "                \"targetIndexName\": index_name,\n",
        "                \"parentKeyFieldName\": \"ParentKey\",\n",
        "                \"sourceContext\": \"/document/chunks/*\",\n",
        "                \"mappings\": [\n",
        "                    {\n",
        "                        \"name\": \"title\",\n",
        "                        \"source\": \"/document/title\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"name\": \"name\",\n",
        "                        \"source\": \"/document/name\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"name\": \"location\",\n",
        "                        \"source\": \"/document/location\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"name\": \"chunk\",\n",
        "                        \"source\": \"/document/chunks/*\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"name\": \"chunkVector\",\n",
        "                        \"source\": \"/document/chunks/*/vector\"\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"parameters\": {\n",
        "            \"projectionMode\": \"skipIndexingParentDocuments\"\n",
        "        }\n",
        "    },\n",
        "    \"cognitiveServices\": {\n",
        "        \"@odata.type\": \"#Microsoft.Azure.Search.CognitiveServicesByKey\",\n",
        "        \"description\": os.environ['COG_SERVICES_NAME'],\n",
        "        \"key\": os.environ['COG_SERVICES_KEY']\n",
        "    }\n",
        "}\n",
        "\n",
        "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/skillsets/\" + skillset_name,\n",
        "                 data=json.dumps(skillset_payload), headers=headers, params=params)\n",
        "print(r.status_code)\n",
        "print(r.ok)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "201\nTrue\n"
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# print(r.text)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and Run the Indexer - (runs the pipeline)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The three components you have created thus far (data source, skillset, index) are inputs to an indexer. Creating the indexer on Azure Cognitive Search is the event that puts the entire pipeline into motion."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an indexer\n",
        "indexer_payload = {\n",
        "    \"name\": indexer_name,\n",
        "    \"dataSourceName\": datasource_name,\n",
        "    \"targetIndexName\": index_name,\n",
        "    \"skillsetName\": skillset_name,\n",
        "    \"schedule\" : { \"interval\" : \"PT30M\"}, # How often do you want to check for new content in the data source\n",
        "    \"fieldMappings\": [\n",
        "        {\n",
        "          \"sourceFieldName\" : \"metadata_title\",\n",
        "          \"targetFieldName\" : \"title\"\n",
        "        },\n",
        "        {\n",
        "          \"sourceFieldName\" : \"metadata_storage_name\",\n",
        "          \"targetFieldName\" : \"name\"\n",
        "        },\n",
        "        {\n",
        "          \"sourceFieldName\" : \"metadata_storage_path\",\n",
        "          \"targetFieldName\" : \"location\"\n",
        "        }\n",
        "    ],\n",
        "    \"outputFieldMappings\":[],\n",
        "    \"parameters\":\n",
        "    {\n",
        "        \"maxFailedItems\": -1,\n",
        "        \"maxFailedItemsPerBatch\": -1,\n",
        "        \"configuration\":\n",
        "        {\n",
        "            \"dataToExtract\": \"contentAndMetadata\",\n",
        "            \"imageAction\": \"generateNormalizedImages\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexers/\" + indexer_name,\n",
        "                 data=json.dumps(indexer_payload), headers=headers, params=params)\n",
        "print(r.status_code)\n",
        "print(r.ok)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "201\nTrue\n"
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment if you find an error\n",
        "# r.text"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: If you get a 400 unauthorize error, make sure that you are using the Azure Search MANAGEMENT KEY, not the QUERY key"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally, get indexer status to confirm that it's running\n",
        "try:\n",
        "    r = requests.get(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexers/\" + indexer_name +\n",
        "                     \"/status\", headers=headers, params=params)\n",
        "    # pprint(json.dumps(r.json(), indent=1))\n",
        "    print(r.status_code)\n",
        "    print(\"Status:\",r.json().get('lastResult').get('status'))\n",
        "    print(\"Items Processed:\",r.json().get('lastResult').get('itemsProcessed'))\n",
        "    print(r.ok)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(\"Wait a few seconds until the process starts and run this cell again.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "200\nStatus: inProgress\nItems Processed: 154\nTrue\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When the indexer finishes running we will have all 9.8k documents indexed in your Search Engine!.**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "- https://learn.microsoft.com/en-us/azure/search/cognitive-search-tutorial-blob\n",
        "- https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search\n",
        "- https://learn.microsoft.com/en-us/azure/search/search-get-started-vector"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEXT\n",
        "In the next notebook 02, we will implement another type of indexing call One-to-Many, in which a single CSV or JSON file can be converted into multiple individual searchable documents in Azure Search. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "vscode": {
      "interpreter": {
        "hash": "9ff083f0c83558f9261023d47a77b9b3eb892c62cdbe066d046abcad1a5edb5c"
      }
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}